{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d72c25a6-65c7-459a-a29a-0575c2c32fcb",
   "metadata": {},
   "source": [
    "Data Science : Data science is the study of data to extract meaning full insights for business purpose \n",
    "Data science is an interdisciplinary field that combines statistical analysis,machine learning & domain knowledge to extract insights and knowledge from structured and unstructured data unabling informed decision making & predictive analytics."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea52294b-653f-4713-bce1-a864b8584504",
   "metadata": {},
   "source": [
    "AI : Artificical Intelligence is a smart application which performs tasks without human interaction.\n",
    "Ex; Robots,Alexa,Recommendation system"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab55c995-48fd-4fc9-91de-33ff5e0cc08f",
   "metadata": {},
   "source": [
    "ML : Machine learning is a subset of AI that involves training algorithm to recognize patterns and make predictions based on data.It provides statistical tool to analyze,visualize,predict model and forecasting \n",
    "Ex;Amazon recommendation system .House price prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f105148-ca11-4ed7-836d-2f36462e6f39",
   "metadata": {},
   "source": [
    "DL : Deep Learning is a subset of ML that artificial neural networks with many layers to model and understand complex pattern and relationship in large dataset.\n",
    "Ex;Image recognition,Face detection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58ecf561-3e7d-4c99-83b9-6ba2ef431411",
   "metadata": {},
   "source": [
    "Type's of Machine Learning :\n",
    "\n",
    "1. supervised machine learning :It is an algorithm which involves training model based on labeled data input and output feature extraction to predict new output or new value \n",
    "Ex ; House price prediction , Student result analysis\n",
    "\n",
    "2. Unsupervised machine learning : It is an algorithm which involves training model based on unlabeled data to group similar data to make clusters \n",
    "Ex; customer segmentation, Salary expectations\n",
    "\n",
    "3. Reinforcement Learning : It is a machine learning algorithm paradigm where an agent learns to make decisions by interacting with environment.\n",
    "Ex; Teaching Robot to navigate ,Mobile games "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dff0e63e-4367-47a5-9be2-534f9d707966",
   "metadata": {},
   "source": [
    "Type's of Supervised machine learning :\n",
    "\n",
    "1. Classification algorithm  : It is a supervised machine learning algorithm the goal is to predict categorical value for given input features or output features which involves categorical value i.e Binary form (0,1) \n",
    "Ex : Pass or Fail,success or Fail ,0 or 1 \n",
    "\n",
    "2. Regression algorithm : It is a supervised machine learning algorithm the goal is to predict numerical value for given input features or output features which involves numerical value.\n",
    "Ex : House price prediction "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93e35790-a11f-4ee1-b25c-e535f59ee864",
   "metadata": {},
   "source": [
    "Feature Engineering : Feature Engineering is the process of transforming raw data into features (input variables) that can be used by machine learning algorithms to improve model performance. It involves creating new features, modifying existing ones, and selecting the most relevant features to enhance the predictive power of a model.\n",
    "\n",
    "Importance of Feature Engineering:\n",
    "1. Improve model performance\n",
    "2. handle complexity "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abb49192-f34f-4835-b171-13af0628367b",
   "metadata": {},
   "source": [
    "Feature Encoding : Data Encoding coverting categorical data to numerical data\n",
    "Different type of feature encoding \n",
    "1. One-Hot-Encoding : Creating binary columns for each category(0,1). # get dummies # imputer\n",
    "2. Label Encoding : Assigning a unique integer to each category # LabelEncoder\n",
    "3. Ordinal Encoding :  Assigning integers to categories with a natural order. (Education level)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5b58940-444c-4875-afe5-9ca18bf5b744",
   "metadata": {},
   "source": [
    "Feature Transformation/ Scaling :  Scaling features to have a specific range or distribution, which can help algorithms converge faster.\n",
    "\n",
    "Standardization : It is a feature scaling  data preprocesing technique used in ml and statistics,it is used when the feature of the dataset are on different scale to bring them to a common scale.It transforms the data to have mean of 0 and std of 1.\n",
    "using z_score technique.The algorithm assumes that the data is normally distributed(Linear,logistic,SVM)\n",
    "\n",
    "z_score=x-mean/std\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler=StandardScaler()\n",
    "data=scaler.fit_transform(data)\n",
    "\n",
    "When to use standardization :\n",
    "1. Gradient Descent-Based Algorithms: Algorithms like linear regression, logistic regression, and neural networks.\n",
    "2. Distance-Based Algorithms: Algorithms like k-nearest neighbors (KNN) and support vector machines (SVM).\n",
    "\n",
    "Normalization : It is a feature scaling  data preprocessing technique used in ml and statistics.It rescales the values of feature to range of (0,1) as min-max-scaling.Normalization is used when you want to ensure that the data fits within a specific range,particulerly in algorithm that do not make assumption about the distribution of data.It is especially useful when features in a dataset have different units or scales. Normalization ensures that each feature contributes equally to the model and can improve the performance of algorithms that are sensitive to feature scaling.\n",
    "\n",
    "x=(x-min(x))/(max(x)-min(x))\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "data=scaler.fit_transform(data)\n",
    "\n",
    "When to use Normalization :\n",
    "1. Gradient Descent-Based Algorithms: Algorithms like linear regression, logistic regression, and neural networks.\n",
    "2. Distance-Based Algorithms:Algorithms like k-nearest neighbors (KNN) and support vector machines (SVM).\n",
    "3. Algorithms Sensitive to Scale: Algorithms like k-nearest neighbors (KNN), support vector machines (SVM), and gradient descent-based methods benefit from normalization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7822a725-e222-43eb-8413-f447422aec77",
   "metadata": {},
   "source": [
    "Feature Extraction : Dimensionality Reduction technique: Reducing the number of features while retaining the essential information.\n",
    "PCA : Principle Component Analysis is a statistical technique used for dimensionality reduction in data analysis and machine learning.which is used to transform a high-dimentional dataset to lower dimentional without retailing its original data.\n",
    "\n",
    "Application of PCA :\n",
    "1. Dimensionality Reduction: Reduces the number of variables in high-dimensional datasets while retaining as much variance as possible.\n",
    "2. Data Visualization: Visualizes high-dimensional data in lower-dimensional space (often 2D or 3D) to explore patterns and relationships.\n",
    "3. Noise Filtering: PCA can filter out noise by retaining only the principal components with significant eigenvalues.\n",
    "4. Feature Extraction: PCA can be used as a feature extraction technique for machine learning algorithms by transforming high-dimensional features into a lower-dimensional space."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e202699-fde2-413c-8e36-4312867fde7a",
   "metadata": {},
   "source": [
    "Spliting Data : spliting data r dividing data dependent and independent variable\n",
    "x=data[] # independent variable , feature input ,predictor \n",
    "y=data[] # dependent variable ,target variable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4f8f8ac-218b-4fe0-8891-0eeb2a2878f8",
   "metadata": {},
   "source": [
    "Dataset is divided into 2 types:\n",
    "1. Training data # minimum 70%-80%\n",
    "2. Testing data # 20%-30%\n",
    "\n",
    "In training data its 2 type's:\n",
    "1. Training data \n",
    "2. Validation data (Cross validation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4cb3a8b-ae71-450d-90ea-41b4b32de94d",
   "metadata": {},
   "source": [
    "Training data : It is used by ml algorithm to build the model and learn relationship between input and output features.\n",
    "training data size should be minimum 70% of data.\n",
    "\n",
    "Testing data : It is separate portion of data which is used to evaluate the performance of ml model which is trained on training data.\n",
    "testing data size should be minimum 20% of data.\n",
    "\n",
    "Validation data : It is subset of training data which is used to evaluate the performance of ml model during the training process.\n",
    "\n",
    "Cross validation : It is statistical technique used to evaluate the performance of machine learning model by partitioning the dataset into subsets,training model on some subsets and testing model on other subsets.\n",
    "\n",
    "Advantages : to get better accuracy and build model.\n",
    "\n",
    "Disadvantages : \n",
    "->Time complexity is huge for training big dataset\n",
    "->Model overfit "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ead553f9-d1b5-4d68-9964-5f4d0cb9a01f",
   "metadata": {},
   "source": [
    "1. Variance :It refers to the change in model when using different portion on training r testing data.Good accuracy low variance and bad accuracy high variance.It can lead to overfitting.\n",
    "2. Bias : Bias refers to the error introduced by approximating a real-world problem with a simplified model that doesn't fully capture the complexity of the data.if the low accuracy it has high bias and high accuracy low bias.it can lead to underfitting\n",
    "3. low bias algorithm : Decision tree,knn,and SVM\n",
    "4. high bias algorithm : Linear regression,logistic regression.\n",
    "\n",
    "Variance-Bias Tradeoff: Balancing model complexity to achieve a good tradeoff between bias and variance. The goal is to find a model that generalizes well to new data by minimizing both bias and variance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbfd2b31-70a0-4b83-b689-ba7ed2d81d5e",
   "metadata": {},
   "source": [
    "Overfitting : When the model performs on training data but fails to perform on test data than overfitting occurs.trainig accuracy high and testing accuracy low \n",
    "1. The difference between actual and predicted value is less.\n",
    "2. Low bias\n",
    "3. High accuracy/variance\n",
    "4. training accuracy 90% and testing accuracy 70%\n",
    "Due to high number of datapoints and feature's\n",
    "\n",
    "Underfitting : When the model not able to perform on both training and testing data than underfitting occurs. both training and testing accuracy will b low.\n",
    "1. The difference between actual and predicted value is more.\n",
    "2. High bias\n",
    "3. Low accuracy/variance\n",
    "4. training accuracy 70% and testing accuracy 50%\n",
    "Due to less number of datapoints and feature's\n",
    "\n",
    "Normalized model : when the model performs on both trainig and testing data.\n",
    "low variance and low bias,training and testing accuracy willl be similer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1adbe448-39b7-47f2-8706-885fc2332d34",
   "metadata": {},
   "source": [
    "Ridge Regression : regularization techniques used to prevent overfitting in linear regression models by adding a penalty to the regression coefficients.adds a penalty equal to the sum of the squared values of the coefficients to the ordinary least squares (OLS) cost function.L2 regularization. ridge = Ridge(alpha=1.0)\n",
    "\n",
    "Lasso Regression :regularization techniques used to prevent overfitting in linear regression models by adding a penalty to the regression coefficients.regularization techniques used to prevent overfitting in linear regression models by adding a penalty to the regression coefficients. lasso = Lasso(alpha=1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa8dc0e1-7d06-41fe-ae15-e2a7d64bca82",
   "metadata": {},
   "source": [
    "Hyper Parameter Tunning : It is variable of ML algorithm that evaluate the performance of ML model is good r not while training dataset.  \n",
    "Hyperparameter tuning is the process of finding the optimal set of hyperparameters for a machine learning model to maximize its performance. Hyperparameters are different from model parameters; they are set before the training process and are not learned from the data.\n",
    "\n",
    "Techniques for Hyperparameter Tuning\n",
    "\n",
    "1. Grid Search:Grid search + Cross validation : it is a technique used in ml to find the optimal hyper parameter for a model\n",
    "->Exhaustively searches over a specified parameter grid.\n",
    "->Evaluates all possible combinations of hyperparameters.\n",
    "->Can be computationally expensive, especially with many hyperparameters and large datasets.\n",
    "\n",
    "2. Random Search:\n",
    "->Samples a fixed number of hyperparameter combinations from the specified parameter space randomly.\n",
    "->Often more efficient than grid search since it explores a larger area of the parameter space with fewer iterations.\n",
    "\n",
    "3. Bayesian Optimization:\n",
    "->Uses probabilistic models to predict the performance of hyperparameters.\n",
    "->Balances exploration of new hyperparameter values with exploitation of known good values.\n",
    "->Efficient for expensive and high-dimensional hyperparameter tuning.\n",
    "\n",
    "4. Gradient-based Optimization:\n",
    "->Uses gradient information to iteratively improve hyperparameters.\n",
    "->Suitable for differentiable hyperparameters.\n",
    "\n",
    "5. Evolutionary Algorithms:\n",
    "->Uses principles from evolutionary biology, such as mutation and selection, to evolve hyperparameters over successive generations.\n",
    "->Useful for complex and high-dimensional hyperparameter spaces.\n",
    "\n",
    "6. Automated Machine Learning (AutoML):\n",
    "->Tools and libraries that automate the hyperparameter tuning process (e.g., Google AutoML, AutoKeras, H2O.ai).\n",
    "->Often use a combination of the above techniques.\n",
    "\n",
    "Advantage's : \n",
    "1. Improved Model Performance:\n",
    "->Optimal Parameters: Finding the best hyperparameters can significantly enhance model accuracy, precision, recall, or other performance metrics.\n",
    "->Generalization: Proper tuning can improve a model's ability to generalize to unseen data, reducing overfitting or underfitting.\n",
    "->To reduce overfitting and get better accuracy.\n",
    "\n",
    "2. Better Feature Utilization:\n",
    "->Feature Selection: Techniques like Lasso regression can automatically select important features during hyperparameter tuning, simplifying the model.\n",
    "->Dimensionality Reduction: Some hyperparameters can control the complexity of the model, effectively reducing dimensionality.\n",
    "\n",
    "Disadvantage's :\n",
    "1. Complexity:\n",
    "->Hyperparameter Space: The space of possible hyperparameters can be vast and complex, making the search for optimal values challenging.\n",
    "->Time-Consuming: The process can be lengthy, especially if the dataset is large or the model is complex.\n",
    "->Time complexity.\n",
    "\n",
    "2. Risk of Overfitting:\n",
    "->Validation Set Overfitting: Overly aggressive tuning can lead to models that are too finely tuned to the validation set, reducing their ability to generalize to truly unseen data.\n",
    "->Bias: Repeatedly evaluating hyperparameters on the same validation set can introduce bias.\n",
    "\n",
    "3. Data Dependency:\n",
    "->Dataset Specificity: Hyperparameters optimized for one dataset may not perform well on a different dataset, limiting the transferability of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2801583-2f42-4881-91f3-e992c6b2b973",
   "metadata": {},
   "source": [
    "Ensemble Technique : \n",
    "Ensemble models are techniques in machine learning where multiple models (often referred to as \"weak learners\" or \"base models\") are combined to produce a single predictive model. The idea is that by combining the predictions of several models, the ensemble can achieve better performance and robustness compared to any individual model.\n",
    "\n",
    "Type's of Ensemble technique:\n",
    "\n",
    "1. Bagging technique : It combines multiple models to create a stronger overall model.Bagging involves training multiple versions of the same model on different subsets of the training data (created using bootstrap sampling), and then averaging their predictions (for regression) or using a majority vote (for classification).\n",
    "Ex : Example: Random Forest, which is an ensemble of decision trees.\n",
    "\n",
    "2. Boosting technique : Boosting is a machine learning technique designed to improve the performance of models by combining the predictions of multiple weak learners to create a strong learner. The primary goal of boosting is to improve the predictive accuracy of model by sequentially adding models that corrects the errors made by previous errors.\n",
    "Weak learners: Haven't learnt much from the training dataset.\n",
    "Ex : AdaBoost, Gradient Boosting, XGBoost, LightGBM, CatBoost."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a0f5885-3a48-4fff-acc2-316c26ab9305",
   "metadata": {},
   "source": [
    "For Regression metrics:\n",
    "1. Mean_squared_Error(MSE)\n",
    "2. Root mean squared error(RMSE)\n",
    "3. Mean absolute error\n",
    "4. R squared\n",
    "5. Adjusted R squared\n",
    "\n",
    "For Classification metrics:\n",
    "1. Accuracy score\n",
    "2. Confusion matrix\n",
    "3. Classification_report\n",
    "4. Precision\n",
    "5. Recall\n",
    "6. F1-score\n",
    "7. support"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe8eb8f5-db56-4893-b7de-1b8834196960",
   "metadata": {},
   "source": [
    "1. Mean Squared Error (MSE): MSE measures the average of the squares of the errors, which are the differences between the actual and predicted values.Lower MSE values indicate better model performance. MSE gives a higher penalty for large errors.\n",
    "\n",
    "2.  Root mean squared error(RMSE):  RMSE is the square root of MSE, providing a measure of the average magnitude of the error.Like MSE, lower RMSE values indicate better model performance. RMSE is in the same units as the target variable, making it easier to interpret.\n",
    "\n",
    "3.  Mean Absolute Error :MAE measures the average of the absolute differences between actual and predicted values.Lower MAE values indicate better model performance. MAE is less sensitive to outliers compared to MSE and RMSE.\n",
    "   \n",
    "4.  R-Squared : R² measures the proportion of the variance in the dependent variable that is predictable from the independent variables.R² values range from 0 to 1, with higher values indicating a better fit. An R² value of 1 means the model explains all the variability in the response data.1-(sum of squared residual (error)/sum of square total)\n",
    "  \n",
    "5.  Adjusted R-Squared: Adjusted R² adjusts the R² value based on the number of predictors in the model, penalizing the addition of irrelevant predictors.Adjusted R² provides a more accurate measure of model performance, especially when comparing models with a different number of predictors.\n",
    "\n",
    "6.  Accuracy score : Accuracy is the ratio of correctly predicted instances to the total instances.Higher accuracy indicates better model performance. However, it can be misleading in imbalanced datasets.\n",
    "   accuracy score= number of correct predictions/Total number of predictions\n",
    "\n",
    "7. Confusion matrix :  A confusion matrix is a table that is often used to describe the performance of a classification model by showing the true positives, true negatives, false positives, and false negatives.Provides detailed insights into the model's performance, showing where the model is making errors.\n",
    "   confusion matrix=TP FN\n",
    "                    FP TN\n",
    "   \n",
    "9. Classification Report : The classification report provides a summary of the precision, recall, F1-score, and support for each class.\n",
    "It is a tool used to evaluate the performance of classification model.It provides the summary of various metrics that help in accesing how well the model is performing in terms of classification.\n",
    "\n",
    "10. Precision : It is the ratio of positive prediction to the total number of positive prediction.Measures the accuracy of positive predictions.\n",
    "   Precision=TP/TP+FP\n",
    "\n",
    "11. Recall : It is the ratio of true positive prediction to the total number of actual positive instances.Measures the ability to correctly identify positive instances.\n",
    "    Recall=TP/TP+FN\n",
    "\n",
    "12. F1 score: It is the harmonis mean of precision and recall.It provides balance between precision and recall.\n",
    "    F1-Score=2× (Precision+Recall/Precision×Recall)\n",
    "​\n",
    "13. Support: The number of actual occurrences of each class in the dataset.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba74ee20-7f40-473c-8f1f-7f08a808bc37",
   "metadata": {},
   "source": [
    "MACHINE LEARNING ALGORIHM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55fd0a67-c1dc-4d62-b426-74dff527b646",
   "metadata": {},
   "source": [
    "Classification Algorithm : output as categorical value\n",
    "1. Logistic Regression \n",
    "2. Decision tree   \n",
    "3. Random Forest \n",
    "4. Support vector classifier\n",
    "5. KNN\n",
    "6. Naive Bayes \n",
    "7. Neural networks\n",
    "8. Gradient Boosting machines\n",
    "9. Adaboost\n",
    "10. XGboost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef4c92c9-d924-41c0-88ed-46786d7ad1e1",
   "metadata": {},
   "source": [
    "Regression Algorithm : output as numerical value \n",
    "1. Linear Regression \n",
    "2. Ridge Regression\n",
    "3. Lasso Regression\n",
    "4. Elastic Net \n",
    "5. Decision tree\n",
    "6. Random Forest\n",
    "7. Support vector Regression\n",
    "8. KNN\n",
    "9. Neural networks\n",
    "10. Gradient Boostinf machines\n",
    "11. Adaboost\n",
    "12. XGboost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fdb19d8-15a9-4524-b29f-584ccc60c16a",
   "metadata": {},
   "source": [
    "SUPERVISED MACHINE LEARNING ALGORITHM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "984f8798-1a7b-4aaa-96e3-0c2b8b8ebefc",
   "metadata": {},
   "source": [
    "LINEAR REGRESSION : It is a supervised machine learning regression algorithm which defines the relationship between dependent and independent variable.It is data analysis technique used to predict value of unknown data using known data.\n",
    "\n",
    "1. simple linear gression : It is a model that describes the relationship between one dependent and independent variable .\n",
    "2. Multilinear regression : It is a model that describes the relationship between one dependent and multiple independent variable.\n",
    "3. Polynomial regression : It is the form of linear regression where the relationship between independent variable X and dependent variable Y is modeled as nth degree polynomial.\n",
    "\n",
    "y=mx+c ( simple linear regression)\n",
    "y=mx+m1x1+m2x2+...C ( multilinear regression)\n",
    "\n",
    "y=dependent variable \n",
    "x=independent variable \n",
    "m=slope of the model (coefficient)\n",
    "c=Intercept\n",
    "\n",
    "The main aim of linear regression is to find the best fit line in such a way that the actual and predicted should be minimal(less).\n",
    "\n",
    "Application : \n",
    "1. Health care analysis\n",
    "2. house price prediction \n",
    "3. marketing\n",
    "4. stock prediction \n",
    "\n",
    "Advantage's : \n",
    "1. Simplicity:\n",
    "->Easy to understand and implement.\n",
    "->Computationally efficient, even for large datasets.\n",
    "->scales to large dataset\n",
    "2. Interpretability:\n",
    "->Coefficients provide insights into the relationship between variables.\n",
    "->Interpretable model outputs.\n",
    "3. Quick to Train:\n",
    "->Training linear regression models is fast.\n",
    "4. Baseline Model:\n",
    "->Serves as a good starting point for more complex models.\n",
    "\n",
    "Disadvantage's: \n",
    "1. Linearity Assumption:\n",
    "->Only models linear relationships. Not suitable for non-linear data.\n",
    "->Needs more feature Engineering \n",
    "->If the true relationship is not linear then it not provides the accurate predictions.\n",
    "2. Sensitivity to Outliers:\n",
    "->Outliers can significantly affect the model.\n",
    "->Overfitting with too many variable.\n",
    "3. Multicollinearity:\n",
    "->High correlation between independent variables can lead to unreliable coefficient estimates.\n",
    "4. Homoscedasticity:\n",
    "->Assumes constant variance of errors, which may not hold in practice."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e00b8b5a-38be-4707-b037-cdf78ac7de61",
   "metadata": {},
   "source": [
    "LOGISTIC REGRESSION : It is a supervised machine learning classification algorithm and it is a statistical technique used for binary classification task.The goal is to predict probability of a  categorical value that have two possible value or outcomes(0,1)(True,False)\n",
    "\n",
    "->Logistic regression uses the sigmoid function to map predicted values to probabilities between 0 and 1.(1/1+e to the power of -z)\n",
    "\n",
    "Application :\n",
    "1. Disease Prediction: Predicting the presence or absence of diseases based on patient characteristics, medical history, and diagnostic test results.\n",
    "2. Medical Diagnostics: Classifying medical conditions such as diabetes, hypertension, or heart disease based on symptoms and medical tests.\n",
    "3. Fraud Detection: Identifying fraudulent transactions or activities based on transactional data, user behavior, and historical patterns.\n",
    "4. Risk Management: Predicting the risk associated with investments, insurance claims, or financial decisions based on market data and economic indicators.\n",
    "5. Student Success Prediction: Predicting academic performance and success factors for students based on demographic information, educational background, and previous academic achievements.\n",
    "\n",
    "Advantage's:\n",
    "1. Simplicity and Interpretability:\n",
    "->Easy to understand and interpret the relationship between features and the probability of the outcome.\n",
    "2. Efficiency:\n",
    "->Computationally efficient and fast to train even with large datasets.\n",
    "3. Probabilistic Output:\n",
    "->Provides probabilities for class membership, which can be useful for decision-making processes.\n",
    "4. Feature Importance:\n",
    "->Coefficients indicate the importance and direction of influence of each feature.\n",
    "5. Handles Binary Classification Well:\n",
    "->Well-suited for binary classification problems and can be extended to multi-class classification using techniques like One-vs-Rest (OvR).\n",
    "\n",
    "Disadvantage's:\n",
    "1. Requires Large Sample Size:\n",
    "->Requires a relatively large sample size for stable and reliable estimates.\n",
    "2. Not Suitable for Non-linear Problems:\n",
    "->Struggles with non-linear relationships unless features are transformed.It may not perform well if the relationship is not linear\n",
    "3. Sensitive to Outliers:\n",
    "->Can be sensitive to outliers, which may affect the coefficients.\n",
    "4. Overfitting with high dimentionality."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5632ea98-0c76-43d4-8180-7d9358fc8e85",
   "metadata": {},
   "source": [
    "Logistic regression is classification problem which gives discrete value as outcome where Linear regression is regression problem which gives continuous value as outcome"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "093227f9-acd3-4084-a681-be1a37022cb9",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a60f356c-1584-4bff-b3f1-2fcccd1e8875",
   "metadata": {},
   "source": [
    "SUPPORT VECTOR MACHINE : It is a supervised machine learning algorithm used for both classification and regression tasks.SVM works by finding the hyperplane that best separates the classes in the feature space. One of the key features of SVM is the use of kernel functions, which enable SVM to perform efficiently in high-dimensional spaces and to handle non-linear decision boundaries..separates best fit line with 2 marginal plane.\n",
    "\n",
    "kernel function : a kernel function that enables the algorithm to operate in a high-dimentional implicit features space without having to compute the co-ordinates of the data in that space explicity.\n",
    "1. Linear kernel : The simplest kernel function, used when the data is linearly separable r when we have large number of features.\n",
    "2. Polynomial function : Allows learning of non-linear models by mapping the original features into polynomial feature space.suitable for dataset where iteration between features are significant.\n",
    "3. RBF kernel(Radial Basis Function): Where we have no prior knowleddge about data,when the data is not linearly separable, and you want to capture the non-linear relationships. This is the default and often the most effective kernel.\n",
    "4. Sigmoid kernel : Can be used as a proxy for neural networks.It is less common and typically used in specific scenarios related to neural network models.\n",
    "\n",
    "Application : \n",
    "1. Stock Market Prediction: Classifying stock price movements based on historical data and other financial indicators.\n",
    "2. Spam Detection: Classifying emails as spam or non-spam.\n",
    "3. Sentiment Analysis: Determining the sentiment of a text as positive, negative, or neutral.\n",
    "4. Document Classification: Categorizing documents into predefined classes (e.g., news articles, scientific papers).\n",
    "5. Disease Diagnosis: Predicting the presence or absence of diseases based on patient data \n",
    "6. Face Detection: Identifying and locating human faces in images.\n",
    "\n",
    "Advantage's:\n",
    "1. Effective in High-Dimensional Spaces:\n",
    "->SVMs perform well in high-dimensional spaces and are effective even when the number of dimensions is greater than the number of samples.\n",
    "2. Versatility with Different Kernels:\n",
    "->SVMs support various kernel functions (linear, polynomial, RBF, sigmoid), enabling them to handle both linear and non-linear relationships.\n",
    "3. Robustness to Overfitting:\n",
    "->With the appropriate choice of kernel and regularization parameters, SVMs are less prone to overfitting, especially in high-dimensional space.\n",
    "4. Clear Margin of Separation:\n",
    "->SVMs maximize the margin between the classes, which can lead to better generalization and performance on unseen data.\n",
    "5. Good Performance with Small and Medium-Sized Datasets:\n",
    "->SVMs can be effective when the dataset is small to medium-sized, where other algorithms might not perform as well.\n",
    "\n",
    "Disadvantage's :\n",
    "1. Computational Complexity:\n",
    "->Training an SVM can be computationally intensive, especially with large datasets. The time complexity can be significant, particularly with non-linear kernels.\n",
    "2. Memory Usage:\n",
    "->SVMs can require large amounts of memory for storing the support vectors and other parameters, especially in high-dimensional spaces.\n",
    "3. Choice of Kernel and Parameters:\n",
    "->The performance of SVMs heavily depends on the choice of kernel function and its parameters (e.g., the regularization parameter C and kernel-specific parameters). Finding the optimal parameters can be challenging and time-consuming.\n",
    "4. Not Suitable for Large Datasets:\n",
    "->SVMs are not well-suited for very large datasets due to their computational and memory requirements.\n",
    "5. Interpretability:\n",
    "->SVM models can be less interpretable compared to other models like decision trees or linear regression, particularly when using complex kernels."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89538407-2e41-4900-80f8-5771cb2d77c2",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "784bae74-0e2d-449c-94f7-fb79babbd289",
   "metadata": {},
   "source": [
    "KNN: K-Nearest Neighbors : It is a instance based supervised machine learning algorithm used for both classification as well as regression tasks.It operates on the principle that similar instances exist in close proximity to each other.\n",
    "\n",
    "Instance-Based Learning: KNN is a type of instance-based learning, meaning it memorizes the training dataset rather than learning a set of parameters. Predictions are made based on the stored instances from the training set.\n",
    "\n",
    "How KNN works\n",
    "1. Data Preparation: Ensure the data is in a suitable format, with features and labels clearly defined. Normalize the data if necessary to ensure that all features contribute equally to the distance calculations.\n",
    "2. Distance Calculation: Calculate the distance between the sample to be classified and all other samples in the training set. Common distance metrics include:\n",
    "->Euclidean distance : The straight line distance bw two points.\n",
    "->Manhattan distance : The sum of absolute difference between the co-ordinates of the points.\n",
    "->Minkowski Distance : Generalization of both Euclidean and Manhattan distances.\n",
    "3. Finding Neighbors : Identify the k samples in the training set that are closest to the sample to be classified.\n",
    "4. Majority Voting (for classification): Assign the class that is most common among the k nearest neighbors to the sample.\n",
    "5. For regression tasks, the output is the average of the values of the k nearest neighbors.\n",
    "Returning the Result: Output the predicted class or value for the sample.\n",
    "\n",
    "Application : \n",
    "1. Recommendation Systems: Used in collaborative filtering to recommend items based on the preferences of similar users.\n",
    "2. Image Recognition: Classifying images based on the similarity to other images in the dataset.\n",
    "3. Anomaly Detection: Identifying outliers in data by comparing the distance to neighboring points.\n",
    "4. Pattern Recognition: Handwriting recognition, speech recognition, and other pattern classification tasks.\n",
    "\n",
    "Advantage's:\n",
    "1. Simplicity: Easy to understand and implement.\n",
    "2. No Training Phase: k-NN is a lazy learner, meaning it doesn’t require a training phase. All computation is deferred until classification.\n",
    "3. Adaptability: Works well with multi-class classification problems.\n",
    "4. Versatility: Can be used for both classification and regression tasks.\n",
    "\n",
    "Disdvantage's :\n",
    "1. Computationally Intensive: The algorithm needs to calculate the distance between the sample and all other samples in the training set, making it slow for large datasets.\n",
    "2. Memory Intensive: Requires storing all training data.\n",
    "3. Sensitive to Irrelevant Features: All features contribute equally to the distance metric, so irrelevant features can negatively impact performance.\n",
    "4. Curse of Dimensionality: Performance can degrade with high-dimensional data due to the sparsity of data points."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4108cc3-c13b-4d2d-ab8e-75ee2705d60b",
   "metadata": {},
   "source": [
    "DECISION TREE : It is a supervised machine learning algorithm  used for both classification and regression tasks.A decision tree is a graphical representation used for decision-making and classification. It consists of nodes, branches, and leaves that collectively form a tree-like structure. It operates by segmenting the data into smaller and smaller groups until each group can be classified or predicted with high degree of accuracy.\n",
    "\n",
    "->pre-prunning(smaller dataset) and post prunning(Large dataset)are used to reduce overfitting by hyper parameter (max_depth,max_feature).\n",
    "\n",
    "Tree Structure: A decision tree consists of nodes. There are three types of nodes:\n",
    "->Root Node: Represents the entire dataset and is the starting point of the tree.\n",
    "->Internal Nodes: Represent the features and make decisions based on feature values.\n",
    "->Leaf Nodes: Represent the outcome (class label or continuous value).\n",
    "\n",
    "Splitting Criteria: At each node, the algorithm selects the feature that best splits the data. Common criteria for splitting include:\n",
    "->Gini Impurity: Measures the frequency at which any element of the dataset would be misclassified if it was randomly labeled.\n",
    "-> Gain (Entropy): Measures the reduction in entropy or uncertainty after the dataset is split.\n",
    "->Mean Squared Error (for regression): Measures the average squared difference between observed actual outcomes and predicted outcomes.\n",
    "\n",
    "->When the dataset is small : Entropy have to select \n",
    "->when the dataset is large : Gini impurity have to select\n",
    "\n",
    "Application :\n",
    "1. Medical Diagnosis: Predicting diseases based on patient symptoms and medical history.\n",
    "2. Customer Segmentation: Segmenting customers based on purchase behavior for targeted marketing.\n",
    "3. Credit Scoring: Assessing the risk of lending money to potential borrowers.\n",
    "4. Fraud Detection: Identifying fraudulent transactions in banking and finance.\n",
    "5. Manufacturing: Quality control and predictive maintenance.\n",
    "\n",
    "Advantages:\n",
    "1. Interpretability: Decision trees are easy to understand and interpret. They can be visualized, making the decision-making process clear.\n",
    "2. No Feature Scaling Required: Decision trees do not require normalization or scaling of features.\n",
    "3. Handle Both Numerical and Categorical Data: Decision trees can handle both types of data without any transformation.\n",
    "4. Non-Parametric: They do not assume any underlying distribution of the data.\n",
    "                            \n",
    "Disadvantage's :\n",
    "1. Overfitting: Decision trees can create overly complex trees that do not generalize well to unseen data. Pruning and setting parameters like maximum depth can mitigate this.\n",
    "2. Instability: Small changes in the data can result in a completely different tree structure.\n",
    "3. Bias Toward Features with More Levels: Decision trees can be biased toward features with many levels, leading to splits that are not necessarily the best.\n",
    "4. Not Optimal for Linear Relationships: Decision trees are not well-suited for capturing linear relationships compared to other algorithms like linear regression."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef5416ed-2cef-4a3c-87c0-8463ade467c4",
   "metadata": {},
   "source": [
    "NAIVE BAYES THEOREM : Naive Bayes theorem is a probabilistic machine learning model used for classification tasks. It is based on Bayes' Theorem, which describes the probability of an event based on prior knowledge of conditions related to the event. \n",
    " It's called \"naive\" because it assumes that the presence (or absence) of a particular feature in a class is unrelated to the presence (or absence) of any other feature.Bayes' Theorem calculates the probability of a hypothesis given prior knowledge and observed data. \n",
    " P(A/B)=P(A).P(B/A)/P(B)\n",
    " P(A∣B) ; Probability of event A given B true.\n",
    " P(A) ; Probability of event A\n",
    " P(B) ; Probability of event B\n",
    " P(B/A) ; Probability of event B given A true.\n",
    "\n",
    "->Types of Naive Bayes Classifiers\n",
    "1. Gaussian Naive Bayes: Assumes that the continuous values associated with each feature are distributed according to a Gaussian (normal) distribution.\n",
    "2. Multinomial Naive Bayes: Typically used for discrete counts (e.g., word frequencies in text classification).\n",
    "3. Bernoulli Naive Bayes: Used when features are binary (e.g., presence or absence of a feature).\n",
    "\n",
    "Application : \n",
    "1. Text Classification: Naive Bayes is widely used for spam filtering, sentiment analysis, and document classification.\n",
    "2. Medical Diagnosis: Predicts the probability of diseases based on symptoms.\n",
    "3. Email Spam Detection: Classifies emails as spam or not spam based on word frequency.\n",
    "4. Sentiment Analysis: Analyzes customer feedback or social media posts to determine sentiment.\n",
    "5. Recommender Systems: Provides recommendations based on user preferences and behavior.\n",
    "6. News Categorization: Classifies news articles into categories like sports, politics, technology, etc.\n",
    "\n",
    "Advantage's :\n",
    "1. Simplicity: Easy to understand and implement.\n",
    "2. Speed: Fast to train and predict, even with large datasets.\n",
    "3. Efficiency: Requires a small amount of training data to estimate the parameters.\n",
    "4. Scalability: Handles large numbers of features efficiently.\n",
    "5. Performance with High Dimensional Data: Performs well in high-dimensional settings such as text classification.\n",
    "6. Works Well with Small Datasets: Effective even with relatively small datasets.\n",
    "7. No Need for Complex Feature Engineering: Simple probability-based approach doesn't require complex transformations of features.\n",
    "\n",
    "Disadvantage's:\n",
    "1. Independence Assumption: Assumes that features are independent, which is rarely true in real-world scenarios. This can affect accuracy.\n",
    "2. Zero Probability Issue: If a category in the dataset is not present during training, it assigns zero probability to it. Smoothing techniques like Laplace smoothing can mitigate this.\n",
    "3. Limited Expressiveness: The model's simplicity means it may not capture complex relationships in the data.\n",
    "4. Bias: Can be biased if the training data is not representative of the actual data distribution.\n",
    "5. Not Suitable for All Problems: Doesn't perform well on datasets with highly correlated features or where the independence assumption is significantly violated.\n",
    "6. Output Probability Interpretation: The predicted probabilities are not always reliable, especially if the independence assumption is violated."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "803b4f37-60bb-4653-baad-19b0d63b525d",
   "metadata": {},
   "source": [
    "RANDOM FOREST CLASSIFIER: It is supervised machine learning ensemble learning method for classification, regression, and other tasks that operates by constructing a multitude of decision trees during training and outputting the class that is the mode of the classes (classification) or mean prediction (regression) of the individual trees.\n",
    "\n",
    "->A random forest classifier is an ensemble learning method that combines multiple decision trees to improve the classification performance\n",
    "->It is perticularly effective for a variety of classification and regression tasks due to its ability to handle large dataset with higher dimensionality,deals with missing value and overfitting.\n",
    "\n",
    "How Random Forest Works\n",
    "1. Bootstrap Sampling: Randomly selects subsets of the training data (with replacement) to build multiple decision trees.\n",
    "2. Random Feature Selection: At each split in a tree, a random subset of features is chosen, and the best feature from this subset is -selected to split the node. This introduces diversity among the trees.\n",
    "3. Voting: For classification tasks, each tree in the forest votes, and the class with the majority vote is the final prediction. For regression tasks, the average prediction of all the trees is taken.\n",
    "\n",
    "Why should we use Random Forest Instead of Decision Tree\n",
    "1. To improve accuracy\n",
    "2. Reduce overfitting \n",
    "3. Better generalization\n",
    "4. Handling high dimensionality\n",
    "5. Generalized model\n",
    "\n",
    "OOB-Score: out of bag data score : The data which is missed to train during training that is oob_score.(validation data)\n",
    "\n",
    "Application :\n",
    "1. Financial Forecasting: Predicts stock prices, credit scoring, and other financial metrics.\n",
    "2. Medical Diagnosis: Assists in predicting disease presence based on patient data.\n",
    "3. Image Classification: Used in computer vision for tasks like object detection and recognition.\n",
    "4. Recommendation Systems: Recommends products based on user behavior and preferences.\n",
    "5. Biological Data Analysis: Analyzes genomic data and other biological datasets for research purposes.\n",
    "6. Fraud Detection: Identifies fraudulent transactions and activities in finance and other industries.\n",
    "7. customer Segmentation: Segments customers based on their purchasing behavior and other characteristics.\n",
    "\n",
    "Advantage's :\n",
    "1. High Accuracy: Tends to achieve high accuracy by combining the predictions of multiple trees, reducing overfitting.\n",
    "2. Robustness to Overfitting: The averaging of multiple trees helps prevent overfitting, especially when there are many trees.\n",
    "3. Handles Large Datasets: Can efficiently handle large datasets with higher dimensionality.\n",
    "4. Versatility: Applicable to both classification and regression problems.\n",
    "5. Feature Importance: Can provide estimates of feature importance, which helps in understanding the data.\n",
    "6. Handles Missing Values: Can handle missing data to some extent by using the median value for regression and the most common value for classification.\n",
    "7. Works Well with Non-linear Data: Capable of capturing non-linear patterns in the data.\n",
    "\n",
    "Disadvantage's:\n",
    "1. Complexity: More complex and computationally intensive compared to simpler models like decision trees or linear regression.\n",
    "2. Slower Predictions: Making predictions can be slower as it involves aggregating results from multiple trees.\n",
    "3. Resource Intensive: Requires more memory and computational resources, especially with large forests and datasets.\n",
    "4. Interpretability: Harder to interpret compared to a single decision tree due to the aggregation of many trees.\n",
    "5. Overfitting on Noisy Data: While it reduces overfitting, if there is a lot of noise in the data, it can still overfit."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "159a2dde-4643-4222-8fc9-9a54509f03ed",
   "metadata": {},
   "source": [
    "GRADIENT BOOSTING: Gradient Boosting is an ensemble machine learning technique for regression and classification problems. It builds models sequentially, with each new model attempting to correct the errors made by the previous models. The key idea is to combine the outputs of many weak learners to create a strong learner.\n",
    "\n",
    "How Gradient Boosting Works\n",
    "1. Initialize the Model: Start with an initial prediction, often the mean of the target values for regression tasks or a simple classification rule for classification tasks.\n",
    "2. Calculate Residuals: Compute the difference between the actual values and the predicted values (residuals).\n",
    "3. Train Weak Learner: Fit a weak learner (usually a decision tree with limited depth) to the residuals. The goal is to capture the pattern of the residuals.\n",
    "4. Update Model: Add the predictions of the weak learner to the previous predictions to improve accuracy.\n",
    "5. Iterate: Repeat steps 2-4 for a predefined number of iterations or until the residuals are minimized.\n",
    "\n",
    "Applications :\n",
    "1. Finance: Credit scoring, risk management, and algorithmic trading.\n",
    "2. Marketing: Customer segmentation, churn prediction, and campaign optimization.\n",
    "3. Healthcare: Disease prediction, patient risk assessment, and personalized medicine.\n",
    "4. E-commerce: Recommendation systems, fraud detection, and inventory management.\n",
    "5. Manufacturing: Predictive maintenance and quality control.\n",
    "\n",
    "Advantages :\n",
    "1. High Accuracy: Often achieves high predictive accuracy.\n",
    "2. Flexibility: Can be used for both regression and classification tasks.\n",
    "3. Handles Various Data Types: Works well with structured and unstructured data.\n",
    "4. Feature Importance: Provides estimates of feature importance.\n",
    "5. Regularization: Includes techniques like shrinkage (learning rate) and subsampling to prevent overfitting.\n",
    "6. Customizable: Allows use of different loss functions, making it versatile for various applications.\n",
    "\n",
    "Disadvantages :\n",
    "1. Computationally Intensive: Training can be slow, especially with large datasets.\n",
    "2. Memory Usage: Requires more memory compared to simpler models.\n",
    "3. Parameter Tuning: Involves multiple hyperparameters that need tuning (e.g., number of trees, learning rate, tree depth).\n",
    "4. Interpretability: Harder to interpret compared to single decision trees.\n",
    "5. Overfitting: Can overfit if the number of boosting rounds is too high or if the model is too complex."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18674fea-a607-42b5-85b1-b103346daaec",
   "metadata": {},
   "source": [
    "ADABOOST : AdaBoost, short for Adaptive Boosting, is an ensemble learning technique that combines multiple weak learners to form a strong learner. It is particularly known for improving the performance of decision trees and other simple models by focusing on the errors made by previous models.\n",
    "\n",
    "How AdaBoost Works\n",
    "1. Initialize Weights: Each training instance is assigned an equal weight at the start.\n",
    "2. Train Weak Learner: A weak learner (e.g., a decision stump) is trained on the weighted dataset.\n",
    "3. Compute Error: The weighted error of the weak learner is calculated.\n",
    "4. Update Weights: The weights of incorrectly classified instances are increased, while the weights of correctly classified instances are decreased. This process ensures that subsequent learners focus more on the difficult instances.\n",
    "5. Combine Learners: The final model is a weighted sum of all the weak learners, where each learner's weight is determined by its accuracy.\n",
    "\n",
    "Applications :\n",
    "1. Image Recognition: Used in facial recognition and object detection.\n",
    "2. Text Classification: Classifies documents, emails, and other text data.\n",
    "3. Customer Churn Prediction: Predicts whether customers will leave a service.\n",
    "4. Medical Diagnosis: Assists in diagnosing diseases based on patient data.\n",
    "5. Fraud Detection: Identifies fraudulent transactions and activities.\n",
    "\n",
    "Advantages of AdaBoost\n",
    "1. Boosts Accuracy: Can significantly improve the accuracy of weak learners.\n",
    "2. Simple and Versatile: Easy to implement and can be combined with various weak learners.\n",
    "3. Feature Importance: Provides insights into feature importance based on the weak learners.\n",
    "4. Adaptive: Focuses on difficult instances, improving model robustness.\n",
    "5. Less Overfitting: Tends to have lower overfitting compared to other boosting methods, especially when combined with simple base learners.\n",
    "\n",
    "Disadvantages:\n",
    "1. Sensitive to Noisy Data: Can be sensitive to noisy data and outliers, as it focuses heavily on difficult instances.\n",
    "2. Computational Cost: Training can be slow, especially with large datasets.\n",
    "3. Requires Weak Learners: Relies on having weak learners that perform slightly better than random guessing.\n",
    "4. Parameter Tuning: Requires careful tuning of hyperparameters (e.g., number of learners, learning rate)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3da244c5-2201-4f6b-b441-8dc4ff30d6c0",
   "metadata": {},
   "source": [
    "XGBOOST : XGBoost is an optimized distributed gradient boosting library designed to be highly efficient, flexible, and portable. It implements machine learning algorithms under the Gradient Boosting framework. XGBoost provides a parallel tree boosting that solves many data science problems in a fast and accurate way.\n",
    "\n",
    "How XGBoost Works\n",
    "1. Gradient Boosting Framework: Like traditional gradient boosting, XGBoost builds trees sequentially, where each new tree tries to correct errors made by the previous trees.\n",
    "2. Regularization: XGBoost includes regularization terms in the objective function to control the complexity of the model and prevent overfitting.\n",
    "3. Sparsity Aware: Efficiently handles sparse data and missing values.\n",
    "4. Weighted Quantile Sketch: Uses a distributed algorithm to handle weighted data, allowing efficient construction of trees.\n",
    "5. Block Structure: Optimized for parallel computation on both single machines and distributed environments.\n",
    "\n",
    "Application : \n",
    "1. Kaggle Competitions: Frequently used in data science competitions for its high performance.\n",
    "2. Finance: Credit scoring, risk assessment, and algorithmic trading.\n",
    "3. Marketing: Customer segmentation, churn prediction, and recommendation systems.\n",
    "4. Healthcare: Disease prediction, patient risk assessment, and personalized medicine.\n",
    "5. Sales Forecasting: Predicting future sales based on historical data.\n",
    "6. Fraud Detection: Identifying fraudulent transactions and activities.\n",
    "\n",
    "Advantage's:\n",
    "1. High Performance: Known for its high accuracy and performance in machine learning competitions.\n",
    "2. Regularization: Built-in L1 and L2 regularization to prevent overfitting.\n",
    "3. Parallel Processing: Supports parallel computation, speeding up training time.\n",
    "4. Handling Missing Values: Efficiently handles missing values.\n",
    "5. Scalability: Can handle large datasets and distributed computing environments.\n",
    "6. Customizable: Allows for custom loss functions and evaluation metrics.\n",
    "7. Feature Importance: Provides feature importance scores.\n",
    "\n",
    "Disadvantage's:\n",
    "1. Complexity: More complex than simpler models like logistic regression or decision trees.\n",
    "2. Parameter Tuning: Requires careful tuning of multiple hyperparameters to achieve optimal performance.\n",
    "3. Computational Cost: Can be computationally expensive, especially with large datasets.\n",
    "4. Memory Usage: Requires significant memory, particularly with large datasets and deep trees."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afc0ca09-592d-468e-a0d7-a375b31bf779",
   "metadata": {},
   "source": [
    "UNSUPERVISED MACHINE LEARNING ALGORITHM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a350d0c6-7159-4293-a7f9-877a1c36908a",
   "metadata": {},
   "source": [
    "clustering : Grouping data points into clusters based on their similarity\n",
    "\n",
    "Clusters : A group of data points that are more similer to each other than to other points in the clusters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7c95f2f-0d97-4c8c-b381-a4f8b13a7dca",
   "metadata": {},
   "source": [
    "KMEAN-CLUSTERING : It is a popular unsupervised machine-learning algorithm used for partitioning a dataset into a set number of clusters K \n",
    "->The algorithm aims to partition the data into K clusters in which each data points belongs to the cluster with the nearest mean(centroid).\n",
    "\n",
    "Centroid : The center of a cluster often calculated as the mean of all points in the cluster.\n",
    "\n",
    "Process:\n",
    "1. initialise the k value : centroids ;\n",
    "2. Assign the nearest data points to clusters to make group\n",
    "3. update centroids by calculating mean \n",
    "4. Repeate the step 2 and 3 until we get pairs single clusters.\n",
    "\n",
    "\n",
    "How do we select the K value \n",
    "WCSS: within the cluster sum of squares\n",
    "\n",
    "using Euclidean distance and square of distance bw points to the nearest centroids\n",
    "which form elbow method \n",
    "\n",
    "The Elbow Method is a technique used to determine the optimal number of clusters in a dataset for clustering algorithms, such as K-means clustering. The goal is to identify the number of clusters that best represents the structure of the data without overfitting.\n",
    "\n",
    "Limitations:\n",
    "The number of clusters k\n",
    "1. k must be specified beforehand.\n",
    "2. Sensitive to the initial placement of centroids.\n",
    "3. Can converge to a local minimum.\n",
    "4. Not suitable for clusters with non-spherical shapes or varying sizes and densities.\n",
    "\n",
    "Application : \n",
    "1. Customer Segmentation: Segmenting customers based on purchasing behavior, demographics, and preferences.\n",
    "2. Image Compression: Reducing the number of colors in an image while maintaining its overall appearance.\n",
    "3. Document Clustering: Grouping similar documents for topic modeling, organizing large datasets, or improving search relevance.\n",
    "4. Biological Data Analysis: Grouping genes or proteins with similar expression patterns or functional characteristics.\n",
    "\n",
    "Advantage's:\n",
    "1. Simplicity and Ease of Implementation:\n",
    "->Easy to understand and implement.\n",
    "->Suitable for beginners in machine learning and data analysis.\n",
    "2. Efficiency:\n",
    "->Computationally efficient for small to medium-sized datasets.\n",
    "->Scales linearly with the number of data points.\n",
    "3. Speed:\n",
    "->Typically converges quickly compared to other clustering algorithms.\n",
    "4. Versatility:\n",
    "->Applicable to various types of data and numerous fields such as marketing, biology, and image processing.\n",
    "\n",
    "Disadvantage's:\n",
    "1. Need for Prior Specification of k:\n",
    "->The number of clusters k:\n",
    "->k must be specified beforehand, which may not be known in advance.\n",
    "2. Sensitivity to Initial Centroid Placement:\n",
    "->Different initial placements of centroids can lead to different clustering results.\n",
    "->The algorithm may converge to a local minimum.\n",
    "3. Assumption of Spherical Clusters:\n",
    "->Assumes clusters are spherical and equally sized, which may not be true for real-world data.\n",
    "->Not suitable for clusters of arbitrary shapes and sizes.\n",
    "4. Scaling Issues:\n",
    "->Performance degrades with high-dimensional data due to the curse of dimensionality.\n",
    "5. Outliers and Noise Sensitivity:\n",
    "->Highly sensitive to outliers and noise, which can skew the results significantly.\n",
    "6. Fixed Number of Clusters:\n",
    "->Not adaptive; does not change the number of clusters dynamically based on the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23a5bba0-6d81-45a1-95c0-7014e1999bb0",
   "metadata": {},
   "source": [
    "HIERARCHICAL CLUSTRING :Hierarchical clustering is a type of unsupervised machine learning algorithm used to group data points into a hierarchy or tree of clusters. Unlike K-means clustering, hierarchical clustering does not require specifying the number of clusters in advance.\n",
    "There are two main types of hierarchical clustering:\n",
    "1. Agglomerative (Bottom-Up):\n",
    "->Starts with each data point as its own cluster.\n",
    "->Iteratively merges the closest pairs of clusters until only one cluster remains or the desired number of clusters is reached.\n",
    "2. Divisive (Top-Down):\n",
    "->Starts with all data points in one cluster.\n",
    "->Iteratively splits the cluster into smaller clusters until each data point is its own cluster or the desired number of clusters is reached.\n",
    "\n",
    "Steps in Agglomerative Hierarchical Clustering\n",
    "1. Calculate the Distance Matrix: Compute the distance between every pair of data points using a distance metric such as Euclidean distance.\n",
    "2. Merge Closest Clusters: Find the pair of clusters with the smallest distance between them and merge them into a single cluster.\n",
    "3. Update the Distance Matrix: Recalculate the distances between the new cluster and all other clusters.\n",
    "4. Repeat: Continue merging the closest pairs of clusters and updating the distance matrix until all data points are in one cluster or the desired number of clusters is achieved.\n",
    "\n",
    "Dendogram : which helps to select number of clusters by selecting the largest vertical line such a way that no horizontal line passes through it.\n",
    "\n",
    "Application :\n",
    "1. Document and Text Clustering:\n",
    "->Application: Organizing documents or articles into a hierarchical structure based on content similarity.\n",
    "2. Market Segmentation:\n",
    "->Application: Segmenting customers based on purchasing behavior, demographics, or preferences.\n",
    "3. Image Segmentation:\n",
    "->Application: Dividing an image into segments based on pixel similarity.\n",
    "4. Customer Support:\n",
    "->Application: Grouping customer support tickets or feedback based on the nature of issues.\n",
    "\n",
    "Advantages :\n",
    "1. No Need to Specify Number of Clusters:\n",
    "->Unlike K-means, there is no need to pre-specify the number of clusters.\n",
    "2. Dendrogram:\n",
    "->The dendrogram provides a visual representation of the data’s hierarchical structure, which can help in choosing the number of clusters.\n",
    "3. Flexibility:\n",
    "->Can use different distance metrics and linkage criteria based on the problem requirements.\n",
    "4. Reproducibility:\n",
    "->Deterministic in nature (especially agglomerative clustering) which means the result is reproducible.\n",
    "\n",
    "Disadvantage's:\n",
    "1. Computational Complexity:\n",
    "->More computationally intensive compared to K-means, especially for large datasets (time complexity more).\n",
    "2. No Reassignment:\n",
    "->Once a merge or split is done, it cannot be undone, potentially leading to suboptimal clustering.\n",
    "3. Scalability:\n",
    "->Not suitable for very large datasets due to high memory and time requirements.\n",
    "4. Sensitive to Noise and Outliers:\n",
    "->Can be significantly affected by the presence of noisy data and outliers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0a7a15a-162b-46e4-946d-6cba8f6d941a",
   "metadata": {},
   "source": [
    "DBSCAN CLUSTERING :BSCAN (Density-Based Spatial Clustering of Applications with Noise) is a powerful clustering algorithm that identifies clusters based on the density of data points. Unlike K-means or hierarchical clustering, DBSCAN does not require the number of clusters to be specified in advance and can handle noise and outliers effective.\n",
    "\n",
    "How DBSCAN Works: DBSCAN groups points into clusters based on their density, defined by two parameters: eps (the maximum distance between two points to be considered neighbors) and minPts (the minimum number of points required to form a dense region). Here’s a step-by-step \n",
    "\n",
    "explanation:\n",
    "1. Core Points: A point is a core point if it has at least minPts neighbors within eps distance.\n",
    "2. Border Points: A point is a border point if it is within eps distance of a core point but has fewer than minPts neighbors.\n",
    "3. Noise Points: A point is a noise point if it is neither a core point nor a border point.\n",
    "4. Cluster Formation: Start with an arbitrary point. If it is a core point, create a new cluster with this point and all its density-reachable points. Repeat for all points in the dataset.\n",
    "\n",
    "Application :\n",
    "1. Social Network Analysis: Application: Detecting communities or groups within social networks based on interaction patterns.\n",
    "2. Astronomy: Grouping stars, galaxies, or other celestial objects based on their spatial distribution.\n",
    "3. Market Basket Analysis: Grouping items frequently bought together in retail datasets.\n",
    "4. Anomaly Detection: Identifying outliers or anomalies in data, such as fraud detection or network security.\n",
    "\n",
    "Advantages :\n",
    "\n",
    "1. No Need to Specify Number of Clusters:\n",
    "->DBSCAN can automatically detect the number of clusters based on the data’s density.\n",
    "2. Identifies Arbitrarily Shaped Clusters:\n",
    "->Unlike K-means, DBSCAN can find clusters of various shapes and sizes.\n",
    "3. Handles Noise and Outliers:\n",
    "->DBSCAN effectively identifies and labels noise points, making it robust to outliers.\n",
    "4. Scalability:\n",
    "->Efficient for large datasets, especially with indexing structures like k-d trees or ball trees.\n",
    "\n",
    "Disadvantage's:\n",
    "\n",
    "1. Parameter Sensitivity:\n",
    "->The performance of DBSCAN heavily depends on the choice of eps and minPts. Choosing the right values can be challenging.\n",
    "2. Not Suitable for Varying Density Clusters:\n",
    "->DBSCAN may struggle with datasets containing clusters with varying densities, as a single eps value may not be suitable for all clusters.\n",
    "3. Computational Complexity:\n",
    "->Although more scalable than hierarchical clustering, it can still be computationally expensive for very large datasets, especially in high-dimensional spaces.\n",
    "4. Distance Metric Dependency:\n",
    "->The results can be significantly affected by the choice of distance metric."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c8ef8bc-4f86-41f4-b4fa-e2b8cd1cdabc",
   "metadata": {},
   "source": [
    "ASSOCIATION RULE: It is a unsupervised machine learning algorithm used in data mining technique for association rule in transactional database.Association rule learning is a rule-based machine learning method for discovering interesting relations between variables in large datasets. It is commonly used in market basket analysis, where the goal is to find associations between items that frequently co-occur in transactions. The most popular algorithm for generating association rules is the Apriori algorithm.\n",
    "\n",
    "The APRIORI algorithm is a classic algorithm used in data mining for learning association rules. It is primarily used for market basket analysis to identify frequent itemsets and generate association rules. The algorithm operates on the principle that any subset of a frequent itemset must also be frequent.\n",
    "\n",
    "Steps of the Apriori Algorithm\n",
    "\n",
    "1. Generate Candidate Itemsets: Begin with single-item itemsets and generate larger itemsets iteratively.\n",
    "\n",
    "2. Prune Infrequent Itemsets: Remove itemsets that do not meet the minimum support threshold.\n",
    "\n",
    "3. Generate Frequent Itemsets: Identify itemsets that meet the minimum support threshold.\n",
    "\n",
    "4. Generate Association Rules: From the frequent itemsets, generate rules that meet the minimum confidence threshold.\n",
    "\n",
    "Support:Support of an itemset is the proportion of transactions in the dataset in which the itemset appears.\n",
    "Support(A)=number of transaction contain A/total number of transaction.\n",
    "\n",
    "Confidence:Confidence of a rule is the proportion of transactions containing the antecedent in which the consequent also appears.\n",
    "Confidence(A->B)=support(AUB)/Support(A)\n",
    "\n",
    "Lift: Lift of a rule is the ratio of the observed support to that expected if A and B were independent.\n",
    "lift(A->B)=support(AUB/support(A)xsupport(B)\n",
    "\n",
    "Application :\n",
    "1. Market Basket Analysis: Identifying items frequently bought together in retail stores.\n",
    "2. Recommendation Systems: Recommending products or content based on user behavior.\n",
    "3. Web Usage Mining: Discovering patterns in web navigation data to optimize website structure and content\n",
    "4. Fraud Detection: Identifying unusual patterns in transactions that may indicate fraudulent activity.\n",
    "5. Healthcare: Finding patterns in patient records to improve diagnosis and treatment.\n",
    "\n",
    "Advantages :\n",
    "1. Interpretability: The generated rules are easy to understand and interpret, making it accessible for non-experts.\n",
    "2. Actionable Insights: Provides direct actionable insights that can be used for decision-making in various domains.\n",
    "3. Scalability: Can handle large datasets efficiently, especially with optimized algorithms like Apriori and FP-Growth.\n",
    "4. Simplicity to implement\n",
    "5. Support for large dataset\n",
    "6. clear output\n",
    "7. Flexibility\n",
    "\n",
    "Disdvantage's:\n",
    "\n",
    "1. Parameter Sensitivity: The performance and relevance of the rules depend heavily on the chosen support and confidence thresholds.\n",
    "2. Complexity with Large Itemsets: Can generate a large number of rules, making it challenging to identify the most useful ones.\n",
    "3. Redundancy: Many rules can be redundant or irrelevant, especially in datasets with many items.\n",
    "4. Requires Discrete Data: Works best with categorical data and may require preprocessing for continuous data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5af20f2-cd87-4b64-a502-f4e1a36f7694",
   "metadata": {},
   "source": [
    "DEEP LEARNING "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f884eca-cf29-4da1-8c4d-00ba281d9189",
   "metadata": {},
   "source": [
    "Deep  learning is a subset of machine learning that involves neural network with many layers (hence 'deep') to model and understand complex pattern in data.It is perticulaerly effective for tasks where traditional algorithm struggle,such as image and speech recognition & NLP(Natural Language Processing) & Game playing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1cbe6f6-edf5-4fdc-9401-161314f5a985",
   "metadata": {},
   "source": [
    " These deep learning's are used for handling large dataset.The AIM of deep learning is to mimic human brain.Neural networks are class of machine learning algorithm inspired by the strucuture and function of human brain.They are designed to recognize patterns,make decisions and solve complex problems by learning from data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a24bbb60-81ed-40a2-853e-3db7d57bbde7",
   "metadata": {},
   "source": [
    "ANN : AN Artificial Neural networks is a computational model inspired by the way biological neural network in the human brain process information.It consists of interconnected nodes organized in layers, which can learn to recognize pattern classify data and make predictions\n",
    "ANN is the key technology in machine learning and deep learning.\n",
    "\n",
    "Perceptron : Type of neural network in Machine Learning(ANN).\n",
    "\n",
    "How ANN work's:\n",
    "\n",
    "1. Input layers: Recieves the initial data.\n",
    "2. Hidden layers : Multiple layers where data is processed through neurons with activation function,these layers captures heirarchical pattern in the data.\n",
    "3. Output layers : Produces the final prediction or classification,\n",
    "\n",
    "weights : Parameters that adjust the input signals.Each connection between neurons has a weight,which determines the importance of input data.\n",
    "\n",
    "loss function : measures the difference between the predicted and actual output\n",
    "mean squared error for regression task.cross entropy loss for classification task.\n",
    "\n",
    "optimization algorithm : Used to minimize the loss function by adjusting the networks weights.\n",
    "\n",
    "Gradient Descent : Updates the weights based on the gradient of the loss function\n",
    "\n",
    "Forward Propagation : Foward propagation is the process in neural network where input data is passed through the layes of network to produce an output.This process involves calculating the weighted sums of inputs at each neuron,applying activation functions and moving data from the input layers to output layers.\n",
    "\n",
    "Backward Propagation : Backward propagation is the process of training neural networks,where the network  weigths are adjusted based on error of the output compared to expected result.It involves calculating the gradient of loss function with respect to each weights by chain rule which helps in minimizing the loss by updating weights using optimization algorithm \n",
    "\n",
    "Activation function : A function applied to the weighted sum of inputs to introduce non-linearity into the networks.\n",
    "some of common activation function sigmoid,Tanh,ReLu,softmax(used in the output layer to produce probability)\n",
    "\n",
    "Epoch : It is an one full iteration over the entire training dataset(1 complete forward and backward propagation).After an epoch every example in the training dataset has been seen once by the model\n",
    "\n",
    "Batches : The dataset is split into several batches,each batch contains a subset of the training data.The model processes one batch at a time the parameters are updated based on the average loss of example in that batch.\n",
    "\n",
    "Conclusions :\n",
    "Forward Propagation : input layers,weights,hidden layers,bias,activation function\n",
    "Backward Propagation : Loss function optimization update the weights\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea9f125f-ba57-4858-a9a0-05c383782efb",
   "metadata": {},
   "source": [
    "Advantage's:\n",
    "\n",
    "1. Ability to Learn Complex Patterns: ANNs can learn and model complex non-linear relationships in data, making them suitable for tasks where traditional algorithms may struggle.\n",
    "\n",
    "2. Adaptability: ANNs can adapt and learn from new data, making them robust in dynamic environments or where data patterns change over time.\n",
    "\n",
    "3. Parallel Processing: ANNs can perform computations in parallel, which can lead to faster processing times for certain tasks compared to sequential algorithms.\n",
    "\n",
    "4. Generalization: ANNs can generalize from training data to make predictions on unseen data, provided they are properly trained and validated.\n",
    "\n",
    "5. Feature Learning: Deep learning architectures (a type of ANN) can automatically learn relevant features from raw data, reducing the need for manual feature engineering.\n",
    " \n",
    "6. Scalability: ANNs can scale with the size of data, leveraging advancements in hardware (like GPUs) to handle large datasets efficiently.\n",
    "\n",
    "Disadvantage's:\n",
    "1. Requires Large Amounts of Data: ANNs, especially deep learning models, often require large amounts of labeled data for training, which can be expensive and time-consuming to collect and annotate.\n",
    "\n",
    "2. Computational Complexity: Training complex ANNs can require significant computational resources, including high-performance GPUs or even specialized hardware, which can be costly.\n",
    "\n",
    "3. Black Box Nature: ANNs, particularly deep models, can be difficult to interpret and understand how decisions are made (i.e., they are often seen as \"black box\" models).\n",
    "\n",
    "4. Overfitting: ANNs, if not properly regularized or validated, can overfit to the training data, meaning they perform well on training data but poorly on unseen test data.\n",
    "\n",
    "5. Hyperparameter Sensitivity: ANNs have many hyperparameters (e.g., number of layers, learning rate) that need to be tuned, which can require expertise and experimentation.\n",
    "\n",
    "6. Lack of Transparency: Due to their complex nature, ANNs may lack transparency and accountability in certain critical applications where understanding the decision-making process is crucial (e.g., healthcare, legal).\n",
    "\n",
    "Application :\n",
    "1. Image and Speech Recognition:\n",
    "2. Natural Language Processing (NLP):\n",
    "3. Autonomous Vehicles:\n",
    "4. Healthcare and Medicine:\n",
    "5. Finance and Business:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "583acb88-8842-4a8b-836f-af8ccbed6041",
   "metadata": {},
   "source": [
    "CNN : Convolutional Neural Network: It is a class of deep learning primarily used for processing and analyzing visual data.CNN are particularly effective for tasks like image classification,object detection and image segmentation because they can automatically and adaptively learn spatial heirarchies of features from input images.\n",
    "      \n",
    "CNN are widely used in image recognition,computer vision and various other application.\n",
    "            \n",
    "Convolutional layer : The convolutional layer is the core building block of a CNN,it applies a set of leanable filters(kernel) to the input data to produce feature map.\n",
    "        \n",
    "Activation function : Introduce non linearity into the model allowing it to learn more complex functions\n",
    "->Apply non-linear transformations to the output of each layer.\n",
    "->Common activation functions include ReLU (Rectified Linear Unit), Sigmoid, and Tanh.\n",
    "->ReLU is the most widely used activation function in CNNs.\n",
    "\n",
    "CNN Architecture:\n",
    "\n",
    "1. Input Layer:\n",
    "->Takes the raw image data as input.\n",
    "->Dimensions are typically height x width x channels (e.g., 32x32x3 for a color image).\n",
    "   \n",
    "2. Convolutional Layers + Activation Functions:\n",
    "->Extract features from the input image.\n",
    "->Multiple convolutional layers are stacked to learn complex patterns.\n",
    "             \n",
    "3. Pooling Layers: Reduce the spatial dimensions of the feature maps while retaining important information.\n",
    "\n",
    "4. Flatten layer : Converts the 2D feature map into 1D feature vector to be fed into fully connected layer.\n",
    "\n",
    "5. Fully Connected Layers: Integrate the features extracted by the convolutional layers to make final predictions.dense layer that performs the final prediction for classification or regression.\n",
    "\n",
    "6. Output Layer: For classification tasks, a softmax activation function is used to output the probabilities for each class.\n",
    "\n",
    "Application :\n",
    "1. Image Classification\n",
    "->Object Recognition: Identifying objects within an image, such as distinguishing between different animals, vehicles, or everyday items.\n",
    "->Scene Classification: Categorizing an entire scene, such as recognizing different types of landscapes or environments (e.g., beaches, forests, cities).\n",
    "        \n",
    "2. Object Detection\n",
    "->Bounding Box Prediction: Identifying and localizing objects within an image by drawing bounding boxes around them.\n",
    "->Face Detection: Recognizing and locating human faces in images or videos.\n",
    "\n",
    "3. Medical Imaging\n",
    "->Disease Diagnosis: Analyzing medical images such as X-rays, MRIs, and CT scans to detect abnormalities, tumors, or other medical conditions.\n",
    "->Histopathology: Examining tissue samples to identify diseases at a cellular level\n",
    "       \n",
    "4. Facial Recognition\n",
    "->Authentication Systems: Verifying identities in security systems, smartphones, and other devices.\n",
    "->Emotion Detection: Analyzing facial expressions to determine emotional states.\n",
    "             \n",
    "5. Natural Language Processing (NLP)\n",
    "->Image Captioning: Generating descriptive captions for images by combining CNNs with Recurrent Neural Networks (RNNs).\n",
    "->Visual Question Answering: Answering questions about the content of an image.\n",
    "      \n",
    "Advantages :\n",
    "1. Automatic Feature Extraction:\n",
    "CNNs automatically learn and extract features from raw images, eliminating the need for manual feature engineering.\n",
    "2. Spatial Hierarchies of Features:\n",
    "Convolutional layers capture spatial hierarchies in images, recognizing patterns and structures at different levels (e.g., edges, textures, objects).\n",
    "3. Translation Invariance:\n",
    "CNNs can recognize objects regardless of their position in the image, making them robust to translations and slight variations.\n",
    "   \n",
    "Disadvantages :\n",
    "1. Computationally Intensive:\n",
    "Training CNNs requires significant computational power and memory, especially for deep networks and large datasets.\n",
    "2. Data Requirements:\n",
    "CNNs typically require large amounts of labeled data to train effectively, which can be a limitation in scenarios with limited data availability\n",
    "3. Lack of Interpretability:\n",
    "CNNs are often considered \"black boxes\" because it can be challenging to understand how they make decisions and which features they prioritize."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "117adce9-9a66-4040-a240-0d95451d0788",
   "metadata": {},
   "source": [
    "RNN : RECURRENT NEURAL NETWORK These are the type of artificial neural network designed for sequential data.Unlike traditional neural network.RNN have connection that form directed cycles,allowing information to perist.\n",
    "\n",
    "->Recurrent Connections: Unlike traditional neural networks, RNNs have connections that loop back on themselves, allowing them to maintain a memory of previous inputs in the sequence.\n",
    "\n",
    "Type of RNN :\n",
    "1. one to one :Passing one input and getting one output.\n",
    "ex: Image classification\n",
    "2. one to many : Passing one input and getting many output.\n",
    "ex: Music generation,Google translation\n",
    "3. Many to one : Passing many input and getting one output.\n",
    "ex: Sentiment analysis,Sale Prediction\n",
    "4. Many to many : Passing many output and getting many output\n",
    "ex: chatbot,Language traslation.\n",
    "\n",
    "Forward Propagation : It refers to the process of passing input data through the network to generate an output.RNN maintain a hidden state that is passed along time steps allowing the network to retain information from previous input.\n",
    "\n",
    "Backward propagation : In RNN it is a process of computing gradient for the networks parameters by propagating errors backward through the network.This is essential for updating the networks weights during training to minimize the loss function.\n",
    "\n",
    "RNN Architecture :\n",
    "1. Initialization : The input layer takes the sequential input data.\n",
    "2. Hidden layer : Processes each element in the sequence,updating its state based on the current input and previous hidden state. RNN have hidden states that get updated at each time step. The hidden state at time t is a function of the input at time t and the hidden state at time t−1.RNN maintain a hidden state that captures information about previous input in the sequence.\n",
    "3. Output layer : Produces the final output for each time step or the entire sequence\n",
    "4. Weight Sharing: The weights of the network are shared across all time steps, making RNNs parameter-efficient for sequential data.\n",
    "5. parameter sharing : some weights are used across all time steps which helps in learning patterns in the data.\n",
    "6. Loss Function: The loss is typically computed over the entire sequence, and the network is trained using backpropagation through time (BPTT).\n",
    "\n",
    "Variants of RNN: \n",
    "\n",
    "1. LSTM : LONG-SHORT-TERM-MEMORY : It is a type of recurrent neural network architecture that is particularly well-suited for sequence prediction problems.LSTM have connections that loop back on themselves,allowing them to maintain a memory of previous inputs.This makes them especially usefull for tasks where the order of the input data matter such as time series prediction,natural language processing,speech recognition.\n",
    "\n",
    "How LSTM works:\n",
    "1. Input Gate: Decides which values from the input should be updated in the memory cell.It uses a sigmoid function to determine hoe much of each component should be updated.\n",
    "2. Forget Gate : Decides what new information to store in the cell state.It also uses a sigmoid function to decide which ports of the cell state should be forgetten.\n",
    "3. Output Gate : Decides what part of the cell state to output as the hidden state.Determines the output of the LSTM cell.It uses a sigmoid function to decide which ports of the cell state to output.\n",
    "\n",
    "2. GRU : Gated Recurrent Unit (GRU) is a variant of the Recurrent Neural Network (RNN) that is designed to solve the vanishing gradient problem and to be more efficient than the Long Short-Term Memory (LSTM) networks. GRUs have a simpler structure than LSTMs but can often achieve similar performance.\n",
    "\n",
    "->Gates: GRUs use two gates to control the flow of information:->\n",
    "->Reset Gate: Determines how much of the previous hidden state to forget.\n",
    "->Update Gate: Determines how much of the previous hidden state to retain and how much of the new input to incorporate.\n",
    "->Hidden State: GRUs maintain and update a single hidden state, which makes them computationally more efficient than LSTMs that have both a cell state and a hidden state.\n",
    "\n",
    "How GRU works:\n",
    "1. Reset Gate:The reset gate decides how much of the previous hidden state to forget.\n",
    "2. Update Gate: The update gate determines how much of the previous hidden state to keep and how much of the new candidate hidden state to use.\n",
    "3. New Hidden State:The new hidden state is calculated using the reset gate.\n",
    "4. Final Hidden State: The final hidden state is a combination of the previous hidden state and the new hidden state, controlled by the update gate.\n",
    "\n",
    "Advantage's:\n",
    "1. Sequential Data Handling:RNNs are specifically designed to handle sequential data, making them ideal for tasks like time series prediction, natural language processing, and speech recognition.\n",
    "2. Parameter Sharing: RNNs use the same parameters (weights) across all time steps, which makes them parameter-efficient and helps in learning temporal patterns.\n",
    "3. Temporal Dependencies: RNNs can model temporal dependencies and relationships between elements of a sequence, capturing context over time.\n",
    "4. Flexibility: They can handle input sequences of varying lengths, making them flexible for different applications.\n",
    "\n",
    "Disadvantage's:\n",
    "1. Long-Term Dependencies:RNNs struggle to learn long-term dependencies due to the vanishing gradient problem, making it hard to capture patterns that span long sequences.\n",
    "2. Training Time: RNNs can be slow to train because each time step requires the previous hidden state, leading to sequential processing that cannot be parallelized easily.\n",
    "3. Complexity: Designing and tuning RNNs can be complex, requiring careful consideration of the architecture and hyperparameters to achieve good performance.\n",
    "4. Resource Intensive: RNNs can be computationally intensive, requiring significant memory and processing power, especially for long sequences and large datasets.\n",
    "\n",
    "Application : \n",
    "1. Natural Language Processing (NLP):\n",
    "->Language modeling\n",
    "->Machine translation\n",
    "->Text generation\n",
    "->Sentiment analysis\n",
    "2. Time Series Analysis:\n",
    "->Stock price prediction\n",
    "->Weather forecasting\n",
    "3. Speech Recognition:\n",
    "->Transcribing audio to text\n",
    "4. Video Analysis:\n",
    "->Action recognition\n",
    "->Video captioning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ecf6ef2-cc32-4319-a468-ced0843c7c51",
   "metadata": {},
   "source": [
    "RECOMMENDATION SYSTEM :It is a type of machine learning system that suggest items to users based on various factors.These items could be products,movies,boooks,articles or any other content that user might be interested in.Recommendation Engine's are widely used in E-commerce streaming services,social media and other platforms to persomalize user experience and increase engagements.\n",
    "\n",
    "Type's of Recommendation system.\n",
    "\n",
    "1. Content-Based Filtering:\n",
    "->Recommends items similar to those the user has liked in the past based on item features.recommends items by analyzing the content of items and user preference.\n",
    "2. Collaborative Filtering:\n",
    "->Recommends items based on the preferences of similar users or the similarity between items. It can be user-based or item-based.\n",
    "->User-Based Collaborative Filtering: Finds users similar to the target user and recommends items they liked.\n",
    "->Item-Based Collaborative Filtering: Finds items similar to those the target user liked and recommends these similar items.\n",
    "->Recommends items by finding similer users.If A & B have similer tastes,item liked by B can be recommends to A.\n",
    "3. Hybrid Systems:\n",
    "Combine content-based and collaborative filtering techniques to leverage the strengths of both methods and provide more accurate recommendations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b942445-ca39-4aa5-b638-96e94e8bdac5",
   "metadata": {},
   "source": [
    "TIME SERIES ANALYSIS :Time series analysis involves methods for analyzing time series data to extract meaningful statistics and other characteristics of the data. It is used for forecasting, anomaly detection, and understanding the underlying structure of the data.\n",
    "Time series in machine learning refers to the sequence of data points collected or recorded at successfull points in time,typically at uniform  intervals.\n",
    "->This type of data is crucial in various domain including finance,economics,weather forecasting,stock market analysis..\n",
    "\n",
    "1. Time Series Data: A sequence of data points collected or recorded at specific time intervals. Examples include stock prices, weather data, and sales figures.\n",
    "2. Stationarity: A stationary time series has statistical properties (mean, variance, autocorrelation) that do not change over time. Stationarity is a crucial assumption for many time series models.\n",
    "3. Trend: The long term progression or direction in the data.It can be downward or upward.\n",
    "4. Seasonality: Regular repeating patterns or cycles in data,often influenced by seasonal factors(e.g., hourly, daily, monthly).\n",
    "5. Level : Average of variable over a fixed period of time.\n",
    "6. Noise : It refers to the variations which occurs due to unpredictable factors and also do not repeate in particuler pattersns.\n",
    "\n",
    "Time Series Forecasting Models\n",
    "1. Autoregressive Integrated Moving Average (ARIMA):\n",
    "Combines autoregression (AR), differencing (I), and moving average (MA) components. Suitable for univariate time series data.\n",
    "2. Seasonal ARIMA (SARIMA):\n",
    "Extends ARIMA to account for seasonality by incorporating seasonal terms.\n",
    "3. Exponential Smoothing (ETS):\n",
    "Methods like Simple Exponential Smoothing, Holt’s Linear Trend Model, and Holt-Winters Seasonal Model.\n",
    "4. Prophet:\n",
    "A forecasting tool developed by Facebook, designed for handling time series data with strong seasonal effects and missing data.\n",
    "5. Long Short-Term Memory (LSTM):\n",
    "A type of recurrent neural network (RNN) capable of learning long-term dependencies, suitable for complex and large time series datasets.\n",
    "\n",
    "Forecasting model building strategy.\n",
    "1. Define goal\n",
    "2. Data collection\n",
    "3. Explore and visualize series\n",
    "4. preprocess data\n",
    "5. partition series\n",
    "6. Apply forecasting method\n",
    "7. Evaluate and compare performance\n",
    "8. Implement forecasting system."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee79122d-78e5-4dc1-ade4-7c6466c6e0ef",
   "metadata": {},
   "source": [
    "Natural Language Processing (NLP) is a branch of artificial intelligence (AI) that focuses on the interaction between computers and humans through natural language. Its goal is to enable computers to understand, interpret, and generate human language in a way that is both meaningful and useful. NLP combines computational linguistics, computer science, and statistics to process and analyze large amounts of natural language data.\n",
    "\n",
    "Application of NLP:\n",
    "1. Machine Translation: Automatically translating text from one language to another (e.g., Google Translate).\n",
    "2. Speech Recognition: Converting spoken language into text (e.g., Siri, Google Assistant).\n",
    "3. Chatbots and Virtual Assistants: Providing automated customer service or personal assistance (e.g., chatbots on websites).\n",
    "4. Text Summarization: Creating concise summaries of long documents.\n",
    "5. Information Retrieval: Finding relevant information from large datasets (e.g., search engines).\n",
    "6. Sentiment Analysis: Analyzing customer reviews to gauge sentiment towards products or services.\n",
    "7. Language Generation: Creating human-like text based on input data (e.g., generating news articles).\n",
    "\n",
    "\n",
    "Text-Preprocessing Techniques : \n",
    "\n",
    "1. Tokenization: Tokenization is a fundamental preprocessing step in Natural Language Processing (NLP) that involves breaking down a text into smaller units called tokens. These tokens can be words, subwords, or characters, depending on the level of granularity needed for a specific NLP task. Example: \"ChatGPT is amazing!\" becomes (\"ChatGPT\", \"is\", \"amazing\", \"!\").\n",
    "2. Stemming: Stemming is a process in Natural Language Processing (NLP) that involves reducing words to their root or base form, typically by removing suffixes. The primary goal of stemming is to group words with similar meanings under a single term, thus simplifying the data and reducing dimensionality. Ex : 'Running': Run\n",
    "3. Lemmatization: Lemmatization is a text preprocessing technique in Natural Language Processing (NLP) that involves reducing a word to its base or dictionary form, known as the lemma. Unlike stemming, which simply truncates words to remove prefixes or suffixes, lemmatization considers the context and the morphological analysis of the words, ensuring that the root form is a meaningful word.\n",
    "4. BOW (Bag of words):Bow model is a fundamental technique in NLP for txt representation.It simplifies txt data into a format that can be used for various machine learning algorithm by transforming the text into a set of words frequencies.\n",
    "   ->if the word is present than its 1.\n",
    "   ->if the word is not present than its 0.\n",
    "5. TF: Term Frequency : Measures how frequently a term qppears in a document.\n",
    "    TF=(Number of times term appears/Total number of term in document)\n",
    "6. IDF : Inverse Document Frequency : Measures how important a term is. While computing TF,all terms are equal imp\n",
    "   IDF=(Number of sentances/No of sentances containing word)\n",
    "\n",
    "Why TFIDF : Filtering common wordds,it reduces the weight of terms that occur very frequently in the corpus and are hence empirically less informative and highlighting rare words "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11fef2ef-8abc-4649-80b3-39423a24ec72",
   "metadata": {},
   "source": [
    "Text-Preprocessing :  Text preprocessing is a crucial step in Natural Language Processing (NLP) that prepares raw text for analysis and modeling.Proper preprocessing improves the quality and performance of NLP model by formatting text into vector as machine understanable language  Here are some common text preprocessing techniques:\n",
    "1. Text Cleaning: Remove any unnecessary characters or symbols from the text. For example, get rid of extra spaces or special symbols.\n",
    "2. Tokenization: Break the text into smaller pieces, like words or sentences, so it’s easier to work with.\n",
    "3. Normalization(lemmatization r stemming ): Make all text lowercase and reduce words to their root forms (e.g., \"running\" to \"run\") to make sure similar words are treated the same.\n",
    "4. Stopword Removal: Remove common, unimportant words like \"and\" or \"the\" that don’t add much meaning.\n",
    "5. Text Normalization: Fix things like spelling errors and handle special characters or numbers if they’re not needed.\n",
    "6. N-gram Generation: Group words into sets (like pairs or triples) to understand the context better.\n",
    "7. Named Entity Recognition (NER): Identify and label important names, dates, and places in the text.\n",
    "8. Part-of-Speech Tagging: Tag each word with its grammatical role, like noun or verb, to understand its function in the sentence.\n",
    "9. Vectorization: Convert text into numbers so a computer can understand it. For example, turning words into a list of numbers or vectors.\n",
    "BOW (Bag of words) and TfIdf(Term Frequency-Inverse Document Frequency)\n",
    "TF(t,d)= Total number of terms in document d/Number of times term t appears in document\n",
    "IDF(t,D)=log( Number of documents containing term t/ Total number of documents D)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcc75aab-2dc1-4a0b-95b8-5e9dbcce840d",
   "metadata": {},
   "source": [
    "Data Science Project Workflow : \n",
    "1. Problem definition : Clearly define the problem or question you aim to answer with data science.\n",
    "2. Data Collection : Gather the data required to address the problem.\n",
    "3. Data Cleaning :  Prepare the data for analysis by handling missing values, duplicates, and errors.\n",
    "4. Exploratory Data Analysis (EDA) :Gain insights into the data through visualization and summary statistics.\n",
    "5. Feature Engineering : Create new features or modify existing ones to improve model performance.\n",
    "6. Model Selection : Choose the appropriate machine learning algorithms for the task.\n",
    "7. Model Building : Train machine learning models on the training data.\n",
    "8. Model Evaluation : Assess the performance of the models using appropriate metrics.\n",
    "9. Model Deployment : Deploy the selected model to a production environment.\n",
    "10. Model Monitoring and Maintenance : Continuously monitor and maintain the model to ensure it remains effective."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
